{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Adjacency Spectral Embedding (MASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Adjacency Spectral Embedding (MASE) is an extension of Adjacency Spectral Embedding (see ASE [tutorial](https://graspy.neurodata.io/tutorials/embedding/adjacencyspectralembed)) for an arbitrary number of graphs.  Once graphs are embedded, the low-dimensional Euclidean representation can be used to visualize the latent positions of vertices, perform inference, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MultipleASE with Stochastic Block Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate how to use the ``MultipleASE`` class, we will use a simple example: two 2-block stochastic block models (SBMs) with different block probabilities.  Indeed, we will use the same example as in the omnibus [tutorial](https://graspy.neurodata.io/tutorials/embedding/omnibus) to facilitate direct comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate two different graphs using stochastic block models (SBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample two 2-block SBMs (undirected, no self-loops) with 50 vertices, each block containing 25 vertices ($n = [25, 25]$), and with the following block probabilities:\n",
    "\n",
    "\\begin{align*}\n",
    "P_1 = \n",
    "\\begin{bmatrix}0.3 & 0.1\\\\\n",
    "0.1 & 0.7\n",
    "\\end{bmatrix},~\n",
    "P_2 = \\begin{bmatrix}0.3 & 0.1\\\\\n",
    "0.1 & 0.3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The only difference between the two is the within-block probability for the second block. We sample $G_1 \\sim \\text{SBM}(n, P_1)$ and $G_2 \\sim \\text{SBM}(n, P_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspy.simulations import sbm\n",
    "\n",
    "n = [25, 25]\n",
    "P1 = [[0.3, 0.1],\n",
    "      [0.1, 0.7]]\n",
    "P2 = [[0.3, 0.1],\n",
    "      [0.1, 0.3]]\n",
    "\n",
    "np.random.seed(8)\n",
    "G1 = sbm(n, P1)\n",
    "G2 = sbm(n, P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the graphs using heatmap\n",
    "\n",
    "We visualize the sampled graphs using the ``heatmap()`` function. ``heatmap()`` will plot the adjacency matrix, where the colors represent the weight of each edge. In this case, we have binary graphs so the values will be either 0 or 1. \n",
    "\n",
    "We see that there is clear block structure to the graphs. Furthermore, the lower right quarter of $G_1$, representing the within-group connections for the second group, is more dense than that of $G_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspy.plot import heatmap\n",
    "\n",
    "g1 = heatmap(G1, figsize=(7,7), title=\"Visualization of Graph 1\")\n",
    "g2 = heatmap(G2, figsize=(7,7), title=\"Visualization of Graph 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed the two graphs using MASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as ASE fits a single input graph to the Random Dot Product Graph (RDPG) model, MASE fits a sample of graphs to the common subspace independent-edge (COSIE) model, which is an extension of RDPG for multiple graphs. The COSIE model assumes that all of the graphs have important shared properties, so all graphs are derived from a common subspace with orthonormal basis $V$.  But, COSIE also allows the connection probabilities between each vertex to vary from one graph to the next, so each graph $A^{(i)}$ has a corresponding score matrix $R^{(i)}$.  As a result, COSIE facilitates comparison due to the common subspace, while retaining sufficient flexibility to represent heterogeneous collections of graphs, due to the different score matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that 2 clusters should be embedded into 2 dimensions, we specify the number of components as 2. Then, the ``fit_transform()`` method returns the estimated invariant subspace $\\hat{V}$, which we can see has 50 entries corresponding to the 50 vertices embedded in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspy.embed import MultipleASE as MASE\n",
    "\n",
    "embedder = MASE(n_components=2)\n",
    "V_hat = embedder.fit_transform([G1, G2])\n",
    "print(V_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Common Latent Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the two graphs have clear block structures, with higher within-block probabilities than between-block, we see two distinct \"clusters\" when we visualize the latent positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(V_hat[:25, 0], V_hat[:25, 1], c=\"blue\", label=\"Block 1\")\n",
    "ax.scatter(V_hat[25:, 0], V_hat[25:, 1], c=\"red\", label=\"Block 2\")\n",
    "ax.legend()\n",
    "_ = ax.set_title(\"Common Latent Subspace from MASE Embedding\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Probability Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ``MultipleASE()`` stores estimates of the individual score matrices $\\hat{R}^{(i)}$ as well as an estimate of the common invariant subspace $\\hat{V}$, we can recover an estimate of the probability matrix for each graph \n",
    "$$\\hat{P}^{(i)} = V R^{(i)} V^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = embedder.scores_\n",
    "R_1 = R[0]\n",
    "R_2 = R[1]\n",
    "\n",
    "V_hat_T = np.transpose(V_hat)\n",
    "P_1_hat = np.matmul(np.matmul(V_hat, R_1), V_hat_T)\n",
    "P_2_hat = np.matmul(np.matmul(V_hat, R_2), V_hat_T)\n",
    "\n",
    "P1_hat_vis = heatmap(P_1_hat, figsize=(7,7), title=\"Visualization of Probability Matrix 1\")\n",
    "P2_hat_vis = heatmap(P_2_hat, figsize=(7,7), title=\"Visualization of Probability Matrix 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Original Block Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see the clear block structure once again.  In order to obtain an estmate of the original block probabilities, we can average across the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_hat_block = np.array([[np.mean(P_1_hat[:25, :25]), np.mean(P_1_hat[:25, 25:])],\n",
    "               [np.mean(P_1_hat[25:, :25]), np.mean(P_1_hat[25:, 25:])]])\n",
    "P2_hat_block = np.array([[np.mean(P_2_hat[:25, :25]), np.mean(P_2_hat[:25, 25:])],\n",
    "               [np.mean(P_2_hat[25:, :25]), np.mean(P_2_hat[25:, 25:])]])\n",
    "print(\"P1_hat_block =\\n\", P1_hat_block)\n",
    "print(\"P2_hat_block =\\n\", P2_hat_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Although we don't get exactly the same block probabilities as we input into the SBM, it's fairly close with only one 50-vertex graph for each!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling More Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recover an even more accurate representation of the two SBMs with more sample graphs.  Let's try with 100 graphs sampled from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = [sbm(n, P1) for i in range(100)]\n",
    "G2 = [sbm(n, P2) for i in range(100)]\n",
    "\n",
    "V_hat = embedder.fit_transform(G1 + G2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Common Latent Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the clustering is even more distinct, as we would expect with more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(V_hat[:25, 0], V_hat[:25, 1], c=\"blue\", label=\"Block 1\")\n",
    "ax.scatter(V_hat[25:, 0], V_hat[25:, 1], c=\"red\", label=\"Block 2\")\n",
    "ax.legend()\n",
    "_ = ax.set_title(\"Common Latent Subspace from MASE Embedding with Many Sampled Graphs\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Probability Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have many samples, we first obtain the estimated probability matrix $\\hat{P}^{(i)} = \\hat{V} \\hat{R}^{(i)} \\hat{V}^T$ for each graph, and then we average across all the graphs generated from the same SBM in order to recover the probabiliity matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain list of P_hats for each graph\n",
    "R = embedder.scores_\n",
    "V_hat_T = np.transpose(V_hat)\n",
    "P_hat = np.array([np.matmul(np.matmul(V_hat, R[i]), V_hat_T) for i in range(len(R))])\n",
    "\n",
    "#Average across all graphs generated from the same distribution.\n",
    "P1_hat = np.mean(P_hat[:100], 0)\n",
    "P2_hat = np.mean(P_hat[100:], 0)\n",
    "\n",
    "#Visualize\n",
    "P1_hat_vis = heatmap(P1_hat, figsize=(7,7), title=\"Visualization of Probability Matrix 1\")\n",
    "P2_hat_vis = heatmap(P2_hat, figsize=(7,7), title=\"Visualization of Probability Matrix 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Original Block Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that our estimates for the block probabilities are closer to the true values with a higher number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_hat_block = np.array([[np.mean(P1_hat[:25, :25]), np.mean(P1_hat[:25, 25:])],\n",
    "               [np.mean(P_1_hat[25:, :25]), np.mean(P_1_hat[25:, 25:])]])\n",
    "P2_hat_block = np.array([[np.mean(P2_hat[:25, :25]), np.mean(P2_hat[:25, 25:])],\n",
    "               [np.mean(P2_hat[25:, :25]), np.mean(P2_hat[25:, 25:])]])\n",
    "print(\"P1_hat_block =\\n\", P1_hat_block)\n",
    "print(\"P2_hat_block =\\n\", P2_hat_block)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
