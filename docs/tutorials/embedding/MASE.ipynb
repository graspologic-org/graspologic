{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Adjacency Spectral Embedding (MASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Adjacency Spectral Embedding (MASE) is an extension of Adjacency Spectral Embedding (see ASE [tutorial](https://graspy.neurodata.io/tutorials/embedding/adjacencyspectralembed)) for an arbitrary number of graphs.  Once graphs are embedded, the low-dimensional Euclidean representation can be used to visualize the latent positions of vertices, perform inference, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating Omnibus Embedding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate how to use the ``MultipleASE`` class, we will use a simple example: two 2-block stochastic block models (SBMs) with different block probabilities.  Indeed, we will use the same models as in the omnibus [tutorial](https://graspy.neurodata.io/tutorials/embedding/omnibus) to facilitate direct comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate two different graphs using stochastic block models (SBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample two 2-block SBMs (undirected, no self-loops) with 50 vertices, each block containing 25 vertices ($n = [25, 25]$), and with the following block connectivity matrices:\n",
    "\n",
    "\\begin{align*}\n",
    "B_1 = \n",
    "\\begin{bmatrix}0.3 & 0.1\\\\\n",
    "0.1 & 0.7\n",
    "\\end{bmatrix},~\n",
    "B_2 = \\begin{bmatrix}0.3 & 0.1\\\\\n",
    "0.1 & 0.3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The only difference between the two is the within-block probability for the second block. We sample $G_1 \\sim \\text{SBM}(n, B_1)$ and $G_2 \\sim \\text{SBM}(n, B_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import sbm\n",
    "\n",
    "n = [25, 25]\n",
    "B1 = [[0.3, 0.1],\n",
    "      [0.1, 0.7]]\n",
    "B2 = [[0.3, 0.1],\n",
    "      [0.1, 0.3]]\n",
    "\n",
    "np.random.seed(8)\n",
    "G1 = sbm(n, B1)\n",
    "G2 = sbm(n, B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the graphs using heatmap\n",
    "\n",
    "We visualize the sampled graphs using the ``heatmap()`` function. ``heatmap()`` will plot the adjacency matrix, where the colors represent the weight of each edge. In this case, we have binary graphs so the values will be either 0 or 1. \n",
    "\n",
    "We see that there is clear block structure to the graphs. Furthermore, the lower right quarter of $G_1$, representing the within-group connections for the second group, is more dense than that of $G_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.plot import heatmap\n",
    "\n",
    "heatmap(G1, figsize=(7,7), title=\"Visualization of Graph 1\", cbar=False)\n",
    "_ = heatmap(G2, figsize=(7,7), title=\"Visualization of Graph 2\", cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed the two graphs using MASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as ASE fits a single input graph to the Random Dot Product Graph (RDPG) model, MASE fits many input graphs to the common subspace independent-edge (COSIE) model, which is an extension of the RDPG model to multiple graphs. The COSIE model assumes that all of the graphs have important shared properties, but can also have meaningful differences.  As a result, COSIE decomposes the set of graphs into a set of shared latent positions $V$ that describe similarities, and a set of score matrices $R^{(i)}$ that describe how each individual graph is different.  Mathematically, for each graph $A^{(i)}$,\n",
    "$$A^{(i)} = VR^{(i)}V^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, 2 clusters can be faithfully represented in 2 dimensions, so we specify the number of components as 2. Then, the ``fit_transform()`` method returns the estimated latent positions $\\hat{V}$, which we can see has 50 entries corresponding to the 50 vertices embedded in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.embed import MultipleASE as MASE\n",
    "\n",
    "embedder = MASE(n_components=2)\n",
    "V_hat = embedder.fit_transform([G1, G2])\n",
    "print(V_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Common Latent Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the two graphs have clear block structures, with higher within-block probabilities than between-block, we see two distinct \"clusters\" when we visualize the latent subspace positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(V_hat[:25, 0], V_hat[:25, 1], c=\"blue\", label=\"Block 1\")\n",
    "ax.scatter(V_hat[25:, 0], V_hat[25:, 1], c=\"red\", label=\"Block 2\")\n",
    "ax.legend(prop={'size':20})\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "_ = ax.set_title(\"Common Latent Subspace from MASE Embedding\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Probability Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ``MultipleASE()`` stores estimates of the individual score matrices $\\hat{R}^{(i)}$ as well as an estimate of the common subspace positions $\\hat{V}$, we can recover an estimate of the probability matrix for each graph \n",
    "$$\\hat{P}^{(i)} = \\hat{V} \\hat{R}^{(i)} \\hat{V}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = embedder.scores_\n",
    "R_1 = R[0]\n",
    "R_2 = R[1]\n",
    "\n",
    "V_hat_T = np.transpose(V_hat)\n",
    "P_1_hat = np.matmul(np.matmul(V_hat, R_1), V_hat_T)\n",
    "P_2_hat = np.matmul(np.matmul(V_hat, R_2), V_hat_T)\n",
    "\n",
    "P1_hat_vis = heatmap(P_1_hat, figsize=(7,7), title=\"Visualization of Probability Matrix 1\", vmin=0, vmax=1)\n",
    "P2_hat_vis = heatmap(P_2_hat, figsize=(7,7), title=\"Visualization of Probability Matrix 2\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Original Block Connectivity Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see the clear block structure once again.  In order to obtain an estmate of the original block connectivity matrices, we can average across the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1_hat = np.array([[np.mean(P_1_hat[:25, :25]), np.mean(P_1_hat[:25, 25:])],\n",
    "               [np.mean(P_1_hat[25:, :25]), np.mean(P_1_hat[25:, 25:])]])\n",
    "B2_hat = np.array([[np.mean(P_2_hat[:25, :25]), np.mean(P_2_hat[:25, 25:])],\n",
    "               [np.mean(P_2_hat[25:, :25]), np.mean(P_2_hat[25:, 25:])]])\n",
    "print(\"B1_hat =\\n\", B1_hat)\n",
    "print(\"B2_hat =\\n\", B2_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Although we don't get exactly the same block probabilities as we input into the SBM, it's fairly close with only one 50-vertex graph for each!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling More Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recover an even more accurate representation of the two SBMs with more sample graphs.  Let's try with 100 graphs sampled from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = [sbm(n, B1) for i in range(100)]\n",
    "G2 = [sbm(n, B2) for i in range(100)]\n",
    "\n",
    "V_hat = embedder.fit_transform(G1 + G2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Common Latent Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the clustering is even more distinct, as we would expect with more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(V_hat[:25, 0], V_hat[:25, 1], c=\"blue\", label=\"Block 1\")\n",
    "ax.scatter(V_hat[25:, 0], V_hat[25:, 1], c=\"red\", label=\"Block 2\")\n",
    "ax.legend(prop={'size':20})\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Common Latent Subspace from MASE Embedding with Many Sampled Graphs\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Probability Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have many samples, we first obtain the estimated probability matrix $\\hat{P}^{(i)} = \\hat{V} \\hat{R}^{(i)} \\hat{V}^T$ for each graph, and then we average across all the graphs generated from the same SBM in order to recover the probabiliity matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain list of P_hats for each graph\n",
    "R = embedder.scores_\n",
    "V_hat_T = np.transpose(V_hat)\n",
    "P_hat = np.array([np.matmul(np.matmul(V_hat, R[i]), V_hat_T) for i in range(len(R))])\n",
    "\n",
    "#Average across all graphs generated from the same distribution.\n",
    "P1_hat = np.mean(P_hat[:100], 0)\n",
    "P2_hat = np.mean(P_hat[100:], 0)\n",
    "\n",
    "#Visualize\n",
    "P1_hat_vis = heatmap(P1_hat, figsize=(7,7), title=\"Visualization of Probability Matrix 1\", vmin=0, vmax=1)\n",
    "P2_hat_vis = heatmap(P2_hat, figsize=(7,7), title=\"Visualization of Probability Matrix 2\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Original Block Connectivity Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that our estimates for the block probabilities are closer to the true values with a higher number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1_hat = np.array([[np.mean(P1_hat[:25, :25]), np.mean(P1_hat[:25, 25:])],\n",
    "               [np.mean(P_1_hat[25:, :25]), np.mean(P_1_hat[25:, 25:])]])\n",
    "B2_hat = np.array([[np.mean(P2_hat[:25, :25]), np.mean(P2_hat[:25, 25:])],\n",
    "               [np.mean(P2_hat[25:, :25]), np.mean(P2_hat[25:, 25:])]])\n",
    "print(\"B1_hat =\\n\", B1_hat)\n",
    "print(\"B1_hat =\\n\", B2_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
