{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a foundational data analysis task, where members of the data set are sorted into groups or \"clusters\" according to measured similarities between the objects. According to some quantitative criteria, members of the same cluster are similar and members of distinct clusters are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graspy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Gaussian Mixture Model (AUTOGMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Automatic Gaussian Mixture Model or AutoGMM is a clustering algorithm that uses Sklearn's hierarchical agglomerative clustering and then Gaussian mixtured model (GMM) fitting. Different combinations of agglomeration, GMM,and cluster numbers are used in the algorithm, and the clustering with the best selection criterion (bic/aic) is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a foundational data analysis task, where members of the data set are sorted into groups or \"clusters\" according to measured similarities between the objects. According to some quantitative criteria, members of the same cluster are similar and members of distinct clusters are different. \n",
    "\n",
    "This algorithm is a Gaussian mixture model (GMM), a statistical model of clustered data that, simply put, is a composition of multiple normal distributions. Each cluster has a weight $w_k$ assigned to it, and the combined probability distribution, $f(x)$, is of the form:\n",
    "\n",
    "$f(x) = \\sum\\limits_{k = 1}^K {w_{k}f_{k}(x)} = \\sum\\limits_{k = 1}^K {\\frac{w_{k}}{(2\\pi)^{\\frac{d}{2}}|\\sum_{k}|^{-\\frac{1}{2}}}e^{[\\frac{1}{2}(x - \\mu_{k})^{T}\\sum_{k}^{-1}(x - \\mu_{k})]}}$\n",
    "\n",
    "where $k$ is the total number of clusters and $d$ is the dimensionality of the data.\n",
    "\n",
    "Expectation Maximization (EM) algorithms are then run to estimate model parameters and the fitted GMM is used to cluster the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example,  where the algorithm uses all possible forms of clustering on a basic set of ten samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspy.cluster.autogmm import AutoGMMCluster\n",
    "\n",
    "# Ex\n",
    "x = np.identity(10)\n",
    "AutoGMM = AutoGMMCluster(min_components=3, affinity=\"all\")\n",
    "AutoGMM.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results and all calculations are presented as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoGMM.results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at a more complex example with carefully created synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import datasets\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from graspy.cluster.autogmm import AutoGMMCluster\n",
    "from graspy.simulations import sbm\n",
    "from graspy.plot import heatmap\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "# Using all forms of clustering\n",
    "ks = [i for i in range(1,21)]\n",
    "affinities = 'all'\n",
    "linkages = 'all'\n",
    "covariance_types='all'\n",
    "    \n",
    "# Creating Synthetic Data\n",
    "x = np.genfromtxt('/home/caseypw/data/synthetic.csv', delimiter=',',skip_header=0)\n",
    "x = x[:,np.arange(1,x.shape[1])]\n",
    "c_true = np.genfromtxt('/home/caseypw/data/synthetic.csv', delimiter=',', usecols = (0),skip_header=0)\n",
    "\n",
    "def make_bic_plots(results,best_cov,best_k_bic,best_bic):\n",
    "    #plot of all BICS*******************************\n",
    "    titles = ['Full','Tied','Diagonal','Spherical']\n",
    "    bics = np.zeros((44,20))\n",
    "    cov_types = ['full','tied','diag','spherical']\n",
    "    for i,cov_type in enumerate(cov_types):\n",
    "        bics[i*11+0,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'none')]['bic/aic'].values\n",
    "        bics[i*11+1,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'euclidean') & (results['linkage'] == 'ward')]['bic/aic'].values\n",
    "        bics[i*11+2,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'euclidean') & (results['linkage'] == 'complete')]['bic/aic'].values\n",
    "        bics[i*11+3,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'euclidean') & (results['linkage'] == 'average')]['bic/aic'].values\n",
    "        bics[i*11+4,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'euclidean') & (results['linkage'] == 'single')]['bic/aic'].values\n",
    "        bics[i*11+5,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'manhattan') & (results['linkage'] == 'complete')]['bic/aic'].values\n",
    "        bics[i*11+6,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'manhattan') & (results['linkage'] == 'average')]['bic/aic'].values\n",
    "        bics[i*11+7,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'manhattan') & (results['linkage'] == 'single')]['bic/aic'].values\n",
    "        bics[i*11+8,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'cosine') & (results['linkage'] == 'complete')]['bic/aic'].values\n",
    "        bics[i*11+9,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'cosine') & (results['linkage'] == 'average')]['bic/aic'].values\n",
    "        bics[i*11+10,:] = -results.loc[(results['covariance_type'] == cov_type) &\n",
    "            (results['affinity'] == 'cosine') & (results['linkage'] == 'single')]['bic/aic'].values\n",
    "\n",
    "\n",
    "    labels = {0:'none',1:'l2/ward',2:'l2/complete',3:'l2/average',4:'l2/single',\n",
    "            5:'l1/complete',6:'l1/average',7:'l1/single',8:'cos/complete',\n",
    "            9:'cos/average',10:'cos/single'}\n",
    "        \n",
    "    fig, ((ax0,ax1),(ax2,ax3)) = plt.subplots(2,2,sharey='row',sharex='col',figsize=(7,6))\n",
    "    for row in np.arange(bics.shape[0]):\n",
    "        if all(bics[row,:]==-np.inf):\n",
    "            continue\n",
    "        if row<=10:\n",
    "            ax0.plot(np.arange(1,len(ks)+1),bics[row,:])\n",
    "        elif row<=21:\n",
    "            ax1.plot(np.arange(1,len(ks)+1),bics[row,:],label=labels[row%11])\n",
    "        elif row<=32:\n",
    "            ax2.plot(np.arange(1,len(ks)+1),bics[row,:])\n",
    "        elif row<=43:\n",
    "            ax3.plot(np.arange(1,len(ks)+1),bics[row,:])\n",
    "    \n",
    "    #plot line indicating chosen model\n",
    "    if best_cov == 'full':\n",
    "        ylims = ax0.get_ylim()\n",
    "        ax0.plot([best_k_bic,best_k_bic],[ylims[0],best_bic],color='black',linestyle='dashed',linewidth=2)\n",
    "    elif best_cov == 'tied':\n",
    "        ylims = ax1.get_ylim()\n",
    "        ax1.plot([best_k_bic,best_k_bic],[ylims[0],best_bic],color='black',linestyle='dashed',linewidth=2)\n",
    "    elif best_cov == 'diag':\n",
    "        ylims = ax2.get_ylim()\n",
    "        ax2.plot([best_k_bic,best_k_bic],[ylims[0],best_bic],color='black',linestyle='dashed',linewidth=2)\n",
    "    elif best_cov == 'spherical':\n",
    "        ylims = ax3.get_ylim()\n",
    "        ax3.plot([best_k_bic,best_k_bic],[ylims[0],best_bic],color='black',linestyle='dashed',linewidth=2)\n",
    "    \n",
    "    fig.text(0.5, 0.04, 'Number of Components', ha='center',fontsize=18,fontweight='bold')\n",
    "    fig.text(0.01, 0.5, 'BIC', va='center', rotation='vertical',fontsize=18,fontweight='bold')\n",
    "    \n",
    "    ax0.set_title(titles[0],fontsize=22,fontweight='bold')\n",
    "    ax0.locator_params(axis='y',tight=True,nbins=4)\n",
    "    ax0.set_yticklabels(ax0.get_yticks(),fontsize=18)\n",
    "\n",
    "    ax1.set_title(titles[1],fontsize=22,fontweight='bold')\n",
    "    legend = ax1.legend(loc='best',bbox_to_anchor=(1.25, 1.25, 0, 0),title='Agglomeration\\nMethod',fontsize=12)\n",
    "    plt.setp(legend.get_title(),fontsize=14)\n",
    "\n",
    "    ax2.set_title(titles[2],fontsize=22,fontweight='bold')\n",
    "    ax2.set_xticks(range(0,21,4))\n",
    "    ax2.set_xticklabels(ax2.get_xticks(),fontsize=18)\n",
    "    ax2.locator_params(axis='y',tight=True,nbins=4)\n",
    "    ax2.set_yticklabels(ax2.get_yticks(),fontsize=18)\n",
    "    \n",
    "\n",
    "    ax3.set_title(titles[3],fontsize=22,fontweight='bold')\n",
    "    ax3.set_xticks(range(0,21,4))\n",
    "    ax3.set_xticklabels(ax3.get_xticks(),fontsize=18)\n",
    "\n",
    "# Run algorithm\n",
    "pyc = AutoGMMCluster(min_components=ks[0],max_components=ks[len(ks)-1],\n",
    "    affinity=affinities,linkage=linkages,covariance_type=covariance_types,\n",
    "    random_state=0)\n",
    "\n",
    "pyc.fit(x,c_true)\n",
    "\n",
    "# Calculated covariates\n",
    "combo = [pyc.affinity_,pyc.linkage_,pyc.covariance_type_]\n",
    "k = pyc.n_components_\n",
    "reg = pyc.reg_covar_\n",
    "bic = -pyc.criter_\n",
    "results = pyc.results_\n",
    "ari = pyc.ari_\n",
    "c_hat_autogmm = pyc.predictions\n",
    "\n",
    "print('Info:')\n",
    "print('Best model: ' + str(combo))\n",
    "print('Best reg: ' + str(reg))\n",
    "print('Best k: ' + str(k))\n",
    "print('Best BIC: ' + str(bic))\n",
    "print('Best ari: ' + str(ari))\n",
    "\n",
    "# BIC Plots\n",
    "make_bic_plots(results,combo[2],k,bic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyc.results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering (kclust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kclust is a clustering algorithm that finds the optimal model by using all algorithms and calculating the lowest silhouette score from Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspy.cluster.kclust import KMeansCluster\n",
    "\n",
    "# Ex\n",
    "x = np.identity(10)\n",
    "KMeansClust = KMeansCluster(max_clusters=5)\n",
    "KMeansClust.fit(x)\n",
    "\n",
    "KMeansClust.model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Synthetic Data\n",
    "x = np.genfromtxt('/home/caseypw/data/synthetic.csv', delimiter=',',skip_header=0)\n",
    "x = x[:,np.arange(1,x.shape[1])]\n",
    "c_true = np.genfromtxt('/home/caseypw/data/synthetic.csv', delimiter=',', usecols = (0),skip_header=0)\n",
    "\n",
    "KMeansClust.fit(x, c_true)\n",
    "\n",
    "KMeansClust.model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraspyClust (gclust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gclust is the last clustering algorithm and it is purely a GMM approach, with no agglomerative clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspy.cluster.gclust import GaussianCluster\n",
    "\n",
    "# Ex\n",
    "x = np.identity(10)\n",
    "GClust = GaussianCluster()\n",
    "GClust.fit(x)\n",
    "\n",
    "GClust.model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Synthetic Data\n",
    "x = np.genfromtxt('/home/caseypw/data/synthetic.csv', delimiter=',',skip_header=0)\n",
    "x = x[:,np.arange(1,x.shape[1])]\n",
    "c_true = np.genfromtxt('/home/caseypw/data/synthetic.csv', delimiter=',', usecols = (0),skip_header=0)\n",
    "\n",
    "GClust.fit(x, c_true)\n",
    "\n",
    "GClust.model_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}