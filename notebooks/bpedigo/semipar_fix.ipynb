{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import graspy as gs\n",
    "from graspy.utils import import_graph, symmetrize, is_symmetric\n",
    "from graspy.inference import BaseInference\n",
    "from graspy.embed import AdjacencySpectralEmbed, LaplacianSpectralEmbed, OmnibusEmbed, select_dimension\n",
    "from scipy.spatial import procrustes\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_from_latent(X, Y=None, rescale=True, loops=True, **kwargs):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    if type(X) is not np.ndarray or type(Y) is not np.ndarray:\n",
    "        raise TypeError('Latent positions must be numpy.ndarray')\n",
    "    if len(X.shape) != 2 or len(Y.shape) != 2:\n",
    "        raise ValueError('Latent positions must have dimension 2 (n_vertices, n_dimensions)')\n",
    "    if X.shape != Y.shape:\n",
    "        raise ValueError('Dimensions of latent positions X and Y must be the same')\n",
    "    \n",
    "    P = np.dot(X, Y.T)\n",
    "    if rescale:\n",
    "        if P.min() < 0:\n",
    "            P = P + P.min()\n",
    "        if P.max() > 1:\n",
    "            P = P / P.max()  \n",
    "    # should this be before or after the rescaling, could give diff answers\n",
    "    if not loops:\n",
    "        P = P - np.diag(np.diag(P))\n",
    "    # doing this regardless of rescale because of machine precision errors\n",
    "    P[P < 0] = 0\n",
    "    P[P > 1] = 1\n",
    "    return P\n",
    "\n",
    "def rdpg_from_p(P, symmetric=True, **kwargs):\n",
    "    if type(P) is not np.ndarray:\n",
    "        raise TypeError('P must be numpy.ndarray')\n",
    "    if len(P.shape) != 2:\n",
    "        raise ValueError('P must have dimension 2 (n_vertices, n_dimensions)')\n",
    "    if P.shape[0] != P.shape[1]:\n",
    "        raise ValueError('P must be a square matrix')\n",
    "    if symmetric:\n",
    "        # can cut down on sampling by ~half \n",
    "        triu_inds = np.triu_indices(P.shape[0])\n",
    "        samples = np.random.binomial(1, P[triu_inds])\n",
    "        A = np.zeros_like(P)\n",
    "        A[triu_inds] = samples\n",
    "        A = symmetrize(A)\n",
    "    else:\n",
    "        A = np.random.binomial(1, P)\n",
    "    return A\n",
    "\n",
    "def rdpg_from_latent(X,\n",
    "                     Y=None,\n",
    "                     rescale=True,\n",
    "                     loops=False,\n",
    "                     symmetric=True,\n",
    "                     **kwargs):\n",
    "    P = p_from_latent(X,Y,**kwargs)\n",
    "    return rdpg_from_p(P, **kwargs)\n",
    "\n",
    "class SemiparametricTest(BaseInference):\n",
    "    \"\"\"\n",
    "    Two sample hypothesis test for the semiparamatric problem of determining\n",
    "    whether two random dot product graphs have the same latent positions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : string, { 'ase' (default), 'lse', 'omnibus'}\n",
    "        String describing the embedding method to use.\n",
    "        Must be one of:\n",
    "        'ase'\n",
    "            Embed each graph separately using adjacency spectral embedding\n",
    "            and use Procrustes to align the embeddings.\n",
    "        'lse'\n",
    "            Embed each graph separately using laplacian spectral embedding\n",
    "            and use Procrustes to align the embeddings.\n",
    "        'omnibus'\n",
    "            Embed all graphs simultaneously using omnibus embedding.\n",
    "\n",
    "    n_components : None (default), or Int\n",
    "        Number of embedding dimensions. If None, the optimal embedding\n",
    "        dimensions are found by the Zhu and Godsi algorithm.\n",
    "\n",
    "    test_case : string, {'rotation (default), 'scalar-rotation', 'diagonal-rotation'}\n",
    "        describes the exact form of the hypothesis to test when using 'ase' or 'lse' \n",
    "        as an embedding method. Ignored if using 'omnibus'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding='ase',\n",
    "                 n_components=None,\n",
    "                 n_bootstraps=1000,\n",
    "                 test_case='rotation',):\n",
    "        if type(n_bootstraps) is not int:\n",
    "            raise TypeError()\n",
    "        if type(test_case) is not str:\n",
    "            raise TypeError()\n",
    "\n",
    "        if n_bootstraps < 1:\n",
    "            raise ValueError('{} is invalid number of bootstraps, must be greater than 1'.format(n_bootstraps))\n",
    "        if test_case not in ['rotation', 'scalar-rotation', 'diagonal-rotation']:\n",
    "            raise ValueError('test_case must be one of \\'rotation\\', \\'scalar-rotation\\',\\'diagonal-rotation\\'')\n",
    "\n",
    "        super().__init__(embedding=embedding, n_components=n_components,)\n",
    "\n",
    "        self.n_bootstraps = n_bootstraps\n",
    "        self.test_case = test_case\n",
    "\n",
    "    def _bootstrap(self, X_hat):\n",
    "        t_bootstrap = np.zeros(self.n_bootstraps)\n",
    "        for i in range(self.n_bootstraps):\n",
    "            P = p_from_latent(X_hat)\n",
    "            A1_simulated = rdpg_from_p(P)\n",
    "            A2_simulated = rdpg_from_p(P)\n",
    "            X1_hat_simulated, X2_hat_simulated = self._embed(A1_simulated,\n",
    "                                                             A2_simulated)\n",
    "            t_bootstrap[i] = self._difference_norm(X1_hat_simulated,\n",
    "                                                   X2_hat_simulated)\n",
    "\n",
    "        return t_bootstrap\n",
    "\n",
    "    def _difference_norm(self, X1, X2):\n",
    "        if self.embedding in ['ase', 'lse']:\n",
    "            if self.test_case == 'rotation':\n",
    "                R = orthogonal_procrustes(X1, X2)[0]\n",
    "                return np.linalg.norm(np.dot(X1, R) - X2)\n",
    "            elif self.test_case == 'scalar-rotation':\n",
    "                R, s = orthogonal_procrustes(X1, X2)\n",
    "                return np.linalg.norm(s / np.sum(X1**2) * np.dot(X1, R) - X2)\n",
    "            elif self.test_case == 'diagonal-rotation':\n",
    "                raise NotImplementedError()\n",
    "                normX1 = np.linalg.norm(X1, axis=1)\n",
    "                normX2 =  np.linalg.norm(X2, axis=1)\n",
    "                normX1[normX1 <= 1e-15] = 1\n",
    "                normX2[normX2 <= 1e-15] = 1\n",
    "                # print(X1)\n",
    "                # print(normX1.shape)\n",
    "                # print(normX1[:, None])\n",
    "                X1 = np.divide(X1, normX1[:, None])\n",
    "                X2 = np.divide(X1, normX2[:, None])\n",
    "                R, s = orthogonal_procrustes(X1, X2)\n",
    "                X2 = np.dot(X2, R)\n",
    "                X2 = s / np.sum(X2**2) * X2\n",
    "                # X2 = X2\n",
    "                return np.linalg.norm(X1 - X2)\n",
    "        else:\n",
    "            return np.linalg.norm(X1 - X2)\n",
    "\n",
    "    def _embed(self, A1, A2):\n",
    "        if self.embedding not in ['ase', 'lse', 'omnibus']:\n",
    "            raise ValueError('Invalid embedding method \"{}\"'.format(self.embedding))\n",
    "        if self.embedding == 'ase':\n",
    "            X1_hat = AdjacencySpectralEmbed(k=self.n_components).fit_transform(A1)\n",
    "            X2_hat = AdjacencySpectralEmbed(k=self.n_components).fit_transform(A2)\n",
    "        elif self.embedding == 'lse':\n",
    "            X1_hat = LaplacianSpectralEmbed(k=self.n_components).fit_transform(A1)\n",
    "            X2_hat = LaplacianSpectralEmbed(k=self.n_components).fit_transform(A2)\n",
    "        elif self.embedding == 'omnibus':\n",
    "            X_hat_compound = OmnibusEmbed(k=self.n_components).fit_transform((A1, A2))\n",
    "            X1_hat = X_hat_compound[:A1.shape[0],:]\n",
    "            X2_hat = X_hat_compound[A2.shape[0]:,:]\n",
    "\n",
    "        return (X1_hat, X2_hat)\n",
    "\n",
    "    def fit(self, A1, A2):\n",
    "        A1 = import_graph(A1)\n",
    "        A2 = import_graph(A2)\n",
    "        if not is_symmetric(A1) or not is_symmetric(A2):\n",
    "            raise NotImplementedError() # TODO asymmetric case\n",
    "        if A1.shape != A2.shape:\n",
    "            raise ValueError('Input matrices do not have matching dimensions')\n",
    "        # need to make sure A1 and A2 will both be embeded in same num dims\n",
    "        # could be an argument for doing this in init but I think it makes sense here\n",
    "        if self.n_components is None:\n",
    "            num_dims1 = select_dimension(A1)[0][-1]\n",
    "            num_dims2 = select_dimension(A2)[0][-1]\n",
    "            self.n_components = max(num_dims1, num_dims2)\n",
    "        X_hats = self._embed(A1, A2)\n",
    "        T_sample = self._difference_norm(X_hats[0], X_hats[1])\n",
    "        T1_bootstrap = self._bootstrap(X_hats[0])\n",
    "        T2_bootstrap = self._bootstrap(X_hats[1])\n",
    "\n",
    "        # Continuity correction - note that the +0.5 causes p > 1 sometimes # TODO \n",
    "        p1 = (len(T1_bootstrap[T1_bootstrap >= T_sample]) + 0.5) / self.n_bootstraps\n",
    "        p2 = (len(T2_bootstrap[T2_bootstrap >= T_sample]) + 0.5) / self.n_bootstraps\n",
    "\n",
    "        p = max(p1, p2)\n",
    "\n",
    "        # TODO : what to store as fields here, or make _private fields\n",
    "        # at least for the sake of testing, I'm going to keep everything\n",
    "\n",
    "        self.T1_bootstrap = T1_bootstrap\n",
    "        self.T2_bootstrap = T2_bootstrap\n",
    "        self.T_sample = T_sample\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.p = p\n",
    "\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
