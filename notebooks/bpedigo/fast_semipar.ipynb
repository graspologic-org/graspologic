{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import numpy as np \n",
    "import graspy as gs\n",
    "from graspy.utils import import_graph, symmetrize, is_symmetric\n",
    "from graspy.inference import BaseInference\n",
    "from graspy.embed import AdjacencySpectralEmbed, LaplacianSpectralEmbed, OmnibusEmbed, select_dimension\n",
    "from scipy.spatial import procrustes\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default RDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_from_latent(X, Y=None, rescale=True, loops=True, **kwargs):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    if type(X) is not np.ndarray or type(Y) is not np.ndarray:\n",
    "        raise TypeError('Latent positions must be numpy.ndarray')\n",
    "    if len(X.shape) != 2 or len(Y.shape) != 2:\n",
    "        raise ValueError('Latent positions must have dimension 2 (n_vertices, n_dimensions)')\n",
    "    if X.shape != Y.shape:\n",
    "        raise ValueError('Dimensions of latent positions X and Y must be the same')\n",
    "    \n",
    "    P = np.dot(X, Y.T)\n",
    "    if rescale:\n",
    "        if P.min() < 0:\n",
    "            P = P + P.min()\n",
    "        if P.max() > 1:\n",
    "            P = P / P.max()  \n",
    "    # should this be before or after the rescaling, could give diff answers\n",
    "    if not loops:\n",
    "        P = P - np.diag(np.diag(P))\n",
    "    # doing this regardless of rescale because of machine precision errors\n",
    "    P[P < 0] = 0\n",
    "    P[P > 1] = 1\n",
    "    return P\n",
    "\n",
    "def rdpg_from_p(P, symmetric=True, **kwargs):\n",
    "    if type(P) is not np.ndarray:\n",
    "        raise TypeError('P must be numpy.ndarray')\n",
    "    if len(P.shape) != 2:\n",
    "        raise ValueError('P must have dimension 2 (n_vertices, n_dimensions)')\n",
    "    if P.shape[0] != P.shape[1]:\n",
    "        raise ValueError('P must be a square matrix')\n",
    "    if symmetric:\n",
    "        # can cut down on sampling by ~half \n",
    "        triu_inds = np.triu_indices(P.shape[0])\n",
    "        samples = np.random.binomial(1, P[triu_inds])\n",
    "        A = np.zeros_like(P)\n",
    "        A[triu_inds] = samples\n",
    "        A = symmetrize(A)\n",
    "    else:\n",
    "        A = np.random.binomial(1, P)\n",
    "    return A\n",
    "\n",
    "def rdpg_from_latent(X, Y=None, rescale=True, loops=False, symmetric=True, **kwargs):\n",
    "    P = p_from_latent(X,Y,**kwargs)\n",
    "    return rdpg_from_p(P, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Semipar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ben Pedigo\n",
    "# bpedigo [at] jhu.edu\n",
    "# 10.18.2018\n",
    "\n",
    "class SemiparametricTest(BaseInference):\n",
    "    \"\"\"\n",
    "    Two sample hypothesis test for the semiparamatric problem of determining\n",
    "    whether two random dot product graphs have the same latent positions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : string, { 'ase' (default), 'lse', 'omnibus'}\n",
    "        String describing the embedding method to use.\n",
    "        Must be one of:\n",
    "        'ase'\n",
    "            Embed each graph separately using adjacency spectral embedding\n",
    "            and use Procrustes to align the embeddings.\n",
    "        'lse'\n",
    "            Embed each graph separately using laplacian spectral embedding\n",
    "            and use Procrustes to align the embeddings.\n",
    "        'omnibus'\n",
    "            Embed all graphs simultaneously using omnibus embedding.\n",
    "\n",
    "    n_components : None (default), or Int\n",
    "        Number of embedding dimensions. If None, the optimal embedding\n",
    "        dimensions are found by the Zhu and Godsi algorithm.\n",
    "\n",
    "    test_case : string, {'rotation (default), 'scalar-rotation', 'diagonal-rotation'}\n",
    "        describes the exact form of the hypothesis to test when using 'ase' or 'lse' \n",
    "        as an embedding method. Ignored if using 'omnibus'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding='ase', n_components=None, n_bootstraps=1000, test_case='rotation',):\n",
    "        if type(n_bootstraps) is not int:\n",
    "            raise TypeError()\n",
    "        if type(test_case) is not str:\n",
    "            raise TypeError()\n",
    "\n",
    "        if n_bootstraps < 1:\n",
    "            raise ValueError('{} is invalid number of bootstraps, must be greater than 1'.format(n_bootstraps))\n",
    "        if test_case not in ['rotation', 'scalar-rotation', 'diagonal-rotation']:\n",
    "            raise ValueError('test_case must be one of \\'rotation\\', \\'scalar-rotation\\',\\'diagonal-rotation\\'')\n",
    "\n",
    "        super().__init__(embedding=embedding, n_components=n_components,)\n",
    "\n",
    "        self.n_bootstraps = n_bootstraps\n",
    "        self.test_case = test_case\n",
    "\n",
    "    def _bootstrap(self, X_hat):\n",
    "        t_bootstrap = np.zeros(self.n_bootstraps)\n",
    "        for i in range(self.n_bootstraps):\n",
    "            P = p_from_latent(X_hat)\n",
    "            A1_simulated = rdpg_from_p(P)\n",
    "            A2_simulated = rdpg_from_p(P)\n",
    "            X1_hat_simulated, X2_hat_simulated = self._embed(A1_simulated, A2_simulated)\n",
    "            t_bootstrap[i] = self._difference_norm(X1_hat_simulated, X2_hat_simulated)\n",
    "\n",
    "        return t_bootstrap\n",
    "\n",
    "    def _difference_norm(self, X1, X2):\n",
    "        if self.embedding in ['ase', 'lse']:\n",
    "            if self.test_case == 'rotation':\n",
    "                R = orthogonal_procrustes(X1, X2)[0]\n",
    "                return np.linalg.norm(np.dot(X1, R) - X2)\n",
    "            elif self.test_case == 'scalar-rotation':\n",
    "                R, s = orthogonal_procrustes(X1, X2)\n",
    "                return np.linalg.norm(s / np.sum(X1**2) * np.dot(X1, R) - X2)\n",
    "            elif self.test_case == 'diagonal-rotation':\n",
    "                raise NotImplementedError()\n",
    "                normX1 = np.linalg.norm(X1, axis=1)\n",
    "                normX2 =  np.linalg.norm(X2, axis=1)\n",
    "                normX1[normX1 <= 1e-15] = 1\n",
    "                normX2[normX2 <= 1e-15] = 1\n",
    "                # print(X1)\n",
    "                # print(normX1.shape)\n",
    "                # print(normX1[:, None])\n",
    "                X1 = np.divide(X1, normX1[:, None])\n",
    "                X2 = np.divide(X1, normX2[:, None])\n",
    "                R, s = orthogonal_procrustes(X1, X2)\n",
    "                X2 = np.dot(X2, R)\n",
    "                X2 = s / np.sum(X2**2) * X2\n",
    "                # X2 = X2\n",
    "                return np.linalg.norm(X1 - X2)\n",
    "            elif self.test_case == 'scalar-diagonal-rotation':\n",
    "                '''\n",
    "                X1 = X1-np.mean(X1, 0)\n",
    "                X2 = X1-np.mean(X2, 0)\n",
    "                normX1 = np.linalg.norm(X1)\n",
    "                normX2 =  np.linalg.norm(X2)\n",
    "                X1 /= normX1\n",
    "                X2 /= normX2\n",
    "                R,s = orthogonal_procrustes(X1, X2)\n",
    "                X2 = np.dot(X2, R.T)*s\n",
    "                return np.linalg.norm(X1 - X2)\n",
    "                '''\n",
    "                mx1, mx2, disparity = procrustes(X1,X2)\n",
    "                return disparity\n",
    "        else:\n",
    "            return np.linalg.norm(X1 - X2)\n",
    "\n",
    "    def _embed(self, A1, A2):\n",
    "        if self.embedding not in ['ase', 'lse', 'omnibus']:\n",
    "            raise ValueError('Invalid embedding method \"{}\"'.format(self.embedding))\n",
    "        if self.embedding == 'ase':\n",
    "            X1_hat = AdjacencySpectralEmbed(k=self.n_components).fit_transform(A1)\n",
    "            X2_hat = AdjacencySpectralEmbed(k=self.n_components).fit_transform(A2)\n",
    "        elif self.embedding == 'lse':\n",
    "            X1_hat = LaplacianSpectralEmbed(k=self.n_components).fit_transform(A1)\n",
    "            X2_hat = LaplacianSpectralEmbed(k=self.n_components).fit_transform(A2)\n",
    "        elif self.embedding == 'omnibus':\n",
    "            X_hat_compound = OmnibusEmbed(k=self.n_components).fit_transform((A1, A2))\n",
    "            X1_hat = X_hat_compound[:A1.shape[0],:]\n",
    "            X2_hat = X_hat_compound[A2.shape[0]:,:]\n",
    "\n",
    "        return (X1_hat, X2_hat)\n",
    "\n",
    "    def fit(self, A1, A2):\n",
    "        A1 = import_graph(A1)\n",
    "        A2 = import_graph(A2)\n",
    "        if not is_symmetric(A1) or not is_symmetric(A2):\n",
    "            raise NotImplementedError() # TODO asymmetric case\n",
    "        if A1.shape != A2.shape:\n",
    "            raise ValueError('Input matrices do not have matching dimensions')\n",
    "        # need to make sure A1 and A2 will both be embeded in same num dims\n",
    "        # could be an argument for doing this in init but I think it makes sense here\n",
    "        if self.n_components is None:\n",
    "            num_dims1 = select_dimension(A1)[0][-1]\n",
    "            num_dims2 = select_dimension(A2)[0][-1]\n",
    "            self.n_components = max(num_dims1, num_dims2)\n",
    "        X_hats = self._embed(A1, A2)\n",
    "        T_sample = self._difference_norm(X_hats[0], X_hats[1])\n",
    "        T1_bootstrap = self._bootstrap(X_hats[0])\n",
    "        T2_bootstrap = self._bootstrap(X_hats[1])\n",
    "\n",
    "        # Continuity correction - note that the +0.5 causes p > 1 sometimes # TODO \n",
    "        p1 = (len(T1_bootstrap[T1_bootstrap >= T_sample]) + 0.5) / self.n_bootstraps\n",
    "        p2 = (len(T2_bootstrap[T2_bootstrap >= T_sample]) + 0.5) / self.n_bootstraps\n",
    "\n",
    "        p = max(p1, p2)\n",
    "\n",
    "        # TODO : what to store as fields here, or make _private fields\n",
    "        # at least for the sake of testing, I'm going to keep everything\n",
    "\n",
    "        self.T1_bootstrap = T1_bootstrap\n",
    "        self.T2_bootstrap = T2_bootstrap\n",
    "        self.T_sample = T_sample\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.p = p\n",
    "\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast Semipar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ben Pedigo\n",
    "# bpedigo [at] jhu.edu\n",
    "# 10.18.2018\n",
    "\n",
    "\n",
    "def difference_norm(X1, X2, embedding='ase', test_case='rotation'):\n",
    "    if embedding in ['ase', 'lse']:\n",
    "        if test_case == 'rotation':\n",
    "            R = orthogonal_procrustes(X1, X2)[0]\n",
    "            return np.linalg.norm(np.dot(X1, R) - X2)\n",
    "        elif test_case == 'scalar-rotation':\n",
    "            R, s = orthogonal_procrustes(X1, X2)\n",
    "            return np.linalg.norm(s / np.sum(X1**2) * np.dot(X1, R) - X2)\n",
    "        elif test_case == 'diagonal-rotation':\n",
    "            raise NotImplementedError()\n",
    "            normX1 = np.linalg.norm(X1, axis=1)\n",
    "            normX2 =  np.linalg.norm(X2, axis=1)\n",
    "            normX1[normX1 <= 1e-15] = 1\n",
    "            normX2[normX2 <= 1e-15] = 1\n",
    "            # print(X1)\n",
    "            # print(normX1.shape)\n",
    "            # print(normX1[:, None])\n",
    "            X1 = np.divide(X1, normX1[:, None])\n",
    "            X2 = np.divide(X1, normX2[:, None])\n",
    "            R, s = orthogonal_procrustes(X1, X2)\n",
    "            X2 = np.dot(X2, R)\n",
    "            X2 = s / np.sum(X2**2) * X2\n",
    "            # X2 = X2\n",
    "            return np.linalg.norm(X1 - X2)\n",
    "        elif test_case == 'scalar-diagonal-rotation':\n",
    "            '''\n",
    "            X1 = X1-np.mean(X1, 0)\n",
    "            X2 = X1-np.mean(X2, 0)\n",
    "            normX1 = np.linalg.norm(X1)\n",
    "            normX2 =  np.linalg.norm(X2)\n",
    "            X1 /= normX1\n",
    "            X2 /= normX2\n",
    "            R,s = orthogonal_procrustes(X1, X2)\n",
    "            X2 = np.dot(X2, R.T)*s\n",
    "            return np.linalg.norm(X1 - X2)\n",
    "            '''\n",
    "            mx1, mx2, disparity = procrustes(X1,X2)\n",
    "            return disparity\n",
    "    else:\n",
    "        return np.linalg.norm(X1 - X2)\n",
    "\n",
    "@nb.jit(parallel=True, fastmath=True)\n",
    "def fast_difference_norm(X1, X2, embedding='ase', test_case='rotation'):\n",
    "    if embedding in ['ase', 'lse']:\n",
    "        if test_case == 'rotation':\n",
    "            R = orthogonal_procrustes(X1, X2)[0]\n",
    "            return np.linalg.norm(np.dot(X1, R) - X2)\n",
    "        elif test_case == 'scalar-rotation':\n",
    "            R, s = orthogonal_procrustes(X1, X2)\n",
    "            return np.linalg.norm(s / np.sum(X1**2) * np.dot(X1, R) - X2)\n",
    "        elif test_case == 'diagonal-rotation':\n",
    "            raise NotImplementedError()\n",
    "            normX1 = np.linalg.norm(X1, axis=1)\n",
    "            normX2 =  np.linalg.norm(X2, axis=1)\n",
    "            normX1[normX1 <= 1e-15] = 1\n",
    "            normX2[normX2 <= 1e-15] = 1\n",
    "            # print(X1)\n",
    "            # print(normX1.shape)\n",
    "            # print(normX1[:, None])\n",
    "            X1 = np.divide(X1, normX1[:, None])\n",
    "            X2 = np.divide(X1, normX2[:, None])\n",
    "            R, s = orthogonal_procrustes(X1, X2)\n",
    "            X2 = np.dot(X2, R)\n",
    "            X2 = s / np.sum(X2**2) * X2\n",
    "            # X2 = X2\n",
    "            return np.linalg.norm(X1 - X2)\n",
    "        elif test_case == 'scalar-diagonal-rotation':\n",
    "            '''\n",
    "            X1 = X1-np.mean(X1, 0)\n",
    "            X2 = X1-np.mean(X2, 0)\n",
    "            normX1 = np.linalg.norm(X1)\n",
    "            normX2 =  np.linalg.norm(X2)\n",
    "            X1 /= normX1\n",
    "            X2 /= normX2\n",
    "            R,s = orthogonal_procrustes(X1, X2)\n",
    "            X2 = np.dot(X2, R.T)*s\n",
    "            return np.linalg.norm(X1 - X2)\n",
    "            '''\n",
    "            mx1, mx2, disparity = procrustes(X1,X2)\n",
    "            return disparity\n",
    "    else:\n",
    "        return np.linalg.norm(X1 - X2)\n",
    "    \n",
    "@nb.jit(parallel=True, fastmath=True)\n",
    "def fast_embed(A1, embedding='ase', n_components=2):\n",
    "    if embedding == 'ase':\n",
    "        X1_hat = AdjacencySpectralEmbed(k=n_components).fit_transform(A1)\n",
    "    elif embedding == 'lse':\n",
    "        X1_hat = LaplacianSpectralEmbed(k=n_components).fit_transform(A1)\n",
    "        X2_hat = LaplacianSpectralEmbed(k=n_components).fit_transform(A2)\n",
    "    elif embedding == 'omnibus':\n",
    "        X_hat_compound = OmnibusEmbed(k=n_components).fit_transform((A1, A2))\n",
    "        X1_hat = X_hat_compound[:A1.shape[0],:]\n",
    "        X2_hat = X_hat_compound[A2.shape[0]:,:]\n",
    "    return X1_hat\n",
    "\n",
    "def embed(A1, A2, embedding='ase', n_components=2):\n",
    "    if embedding not in ['ase', 'lse', 'omnibus']:\n",
    "        raise ValueError('Invalid embedding method \"{}\"'.format(self.embedding))\n",
    "    if embedding == 'ase':\n",
    "        X1_hat = AdjacencySpectralEmbed(k=n_components).fit_transform(A1)\n",
    "        X2_hat = AdjacencySpectralEmbed(k=n_components).fit_transform(A2)\n",
    "    elif embedding == 'lse':\n",
    "        X1_hat = LaplacianSpectralEmbed(k=n_components).fit_transform(A1)\n",
    "        X2_hat = LaplacianSpectralEmbed(k=n_components).fit_transform(A2)\n",
    "    elif embedding == 'omnibus':\n",
    "        X_hat_compound = OmnibusEmbed(k=n_components).fit_transform((A1, A2))\n",
    "        X1_hat = X_hat_compound[:A1.shape[0],:]\n",
    "        X2_hat = X_hat_compound[A2.shape[0]:,:]\n",
    "\n",
    "    return (X1_hat, X2_hat)\n",
    "\n",
    "\n",
    "@nb.jit(parallel=True, fastmath=True)\n",
    "def fast_bootstrap(X_hat, n_bootstraps):\n",
    "    t_bootstrap = np.zeros(n_bootstraps)\n",
    "    for i in range(n_bootstraps):\n",
    "        P = fast_p_from_latent(X_hat, X_hat)\n",
    "        A1_simulated = fast_rdpg_from_p(P)\n",
    "        A2_simulated = fast_rdpg_from_p(P)\n",
    "        X1_hat_simulated = fast_embed(A1_simulated)\n",
    "        X2_hat_simulated = fast_embed(A2_simulated)\n",
    "        t_bootstrap[i] = fast_difference_norm(X1_hat_simulated, X2_hat_simulated)\n",
    "    return t_bootstrap\n",
    "\n",
    "def bootstrap(X_hat, n_bootstraps):\n",
    "    t_bootstrap = np.zeros(n_bootstraps)\n",
    "    for i in range(n_bootstraps):\n",
    "        P = p_from_latent(X_hat)\n",
    "        A1_simulated = rdpg_from_p(P)\n",
    "        A2_simulated = rdpg_from_p(P)\n",
    "        X1_hat_simulated, X2_hat_simulated = embed(A1_simulated, A2_simulated)\n",
    "        t_bootstrap[i] = difference_norm(X1_hat_simulated, X2_hat_simulated)\n",
    "    return t_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, parallel=True)\n",
    "def fast_p_from_latent(X, Y=None, rescale=True, loops=True):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    P = np.dot(X, Y.T)\n",
    "    if rescale:\n",
    "        if P.min() < 0:\n",
    "            P = P + P.min()\n",
    "        if P.max() > 1:\n",
    "            P = P / P.max()  \n",
    "    if not loops:\n",
    "        P = P - np.diag(np.diag(P))\n",
    "    return P\n",
    "\n",
    "@nb.jit(nopython=True) # parallel doesn't seem to work here\n",
    "def fast_rdpg_from_p(P, symmetric=True, loops=True):\n",
    "    m, n = P.shape\n",
    "    P = np.ravel(P)\n",
    "    A = np.ones_like(P)\n",
    "    for i, prob in enumerate(P):\n",
    "        A[i] = np.random.binomial(1, prob)\n",
    "    A = np.reshape(A, (n,n))\n",
    "    if symmetric:\n",
    "        A = fast_symmetrize(A)\n",
    "    return A\n",
    "\n",
    "@nb.jit(nopython=True, parallel=True)\n",
    "def fast_symmetrize(graph):\n",
    "    n = graph.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            graph[j, i] = graph[i, j]\n",
    "    return graph\n",
    "\n",
    "@nb.jit(nopython=True, parallel=True)\n",
    "def fast_rdpg_from_latent(X, Y, rescale=True, loops=False, symmetric=True,):\n",
    "    P = fast_p_from_latent(X,Y, rescale=rescale, loops=loops)\n",
    "    return fast_rdpg_from_p(P, symmetric=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "95.6 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "351 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "115 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "425 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "206 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "1.01 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "661 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "3.74 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "2.53 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "28.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "18.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "132 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "83.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "699 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "397 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "3.38 s ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "1.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "24.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n",
      "9.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_verts = 1000\n",
    "n_dims = 10\n",
    "X = np.random.uniform(size=(10,n_dims))\n",
    "fast_rdpg_from_latent(X,X)\n",
    "\n",
    "X = np.random.uniform(size=(n_verts,n_dims))\n",
    "# run this once to get the first compilation for fair comparison\n",
    "normal = []\n",
    "fast = []\n",
    "sizes = np.logspace(1,4,num=10,dtype=int)\n",
    "for n_verts in sizes:\n",
    "    X = np.random.uniform(size=(n_verts,n_dims))\n",
    "    n = %timeit -r 1 -n 10 -o rdpg_from_latent(X, X)\n",
    "    f = %timeit -r 1 -n 10 -o fast_rdpg_from_latent(X, X)\n",
    "    normal.append(n)\n",
    "    fast.append(f)\n",
    "\n",
    "for i in range(len(normal)):\n",
    "    t_normal = np.mean(normal[i].all_runs)\n",
    "    t_fast = np.mean(fast[i].all_runs)\n",
    "    plt.scatter(sizes[i],t_normal)\n",
    "    plt.scatter(sizes[i],t_fast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 2s ± 2.74 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "45.8 s ± 998 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "n_verts = 1000\n",
    "n_dims = 10\n",
    "n_sims = 100\n",
    "X = np.random.uniform(size=(n_verts,n_dims))\n",
    "fast_bootstrap(X, 2)\n",
    "\n",
    "%timeit bootstrap(X,n_sims)\n",
    "%timeit fast_bootstrap(X, n_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t_normal = np.zeros(n_sims)\n",
    "# t_fast = np.zeros(n_sims)\n",
    "# for i in range(n_sims):\n",
    "#     t = time.process_time()\n",
    "#     A1 = rdpg_from_latent(X)\n",
    "#     t_normal[i] = t - time.process_time()\n",
    "    \n",
    "#     t = time.process_time()\n",
    "#     A2 = fast_rdpg_from_latent(X,X)\n",
    "\n",
    "#     t = time.process_time()\n",
    "#     A3 = fast_rdpg_from_latent(X,X)\n",
    "# t = time.process_time()\n",
    "# spt = SemiparametricTest(n_components=2, n_bootstraps=500,)\n",
    "# spt.fit(A1, A2)\n",
    "# print('Normal')\n",
    "# print('{0:.3f} mins'.format(((time.process_time() - t)/60)))\n",
    "\n",
    "# t = time.process_time()\n",
    "# fast_bootstrap(X, n_bootstraps)\n",
    "# print('First Fast')\n",
    "# print('{0:.3f} mins'.format(((time.process_time() - t)/60)))\n",
    "\n",
    "# t = time.process_time()\n",
    "# spt = FastSemiparametricTest(n_components=2, n_bootstraps=500,)\n",
    "# spt.fit(A1, A2)\n",
    "# print('Second Fast')\n",
    "# print('{0:.3f} mins'.format(((time.process_time() - t)/60)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
